{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1da3b67",
   "metadata": {},
   "source": [
    "# Exercise 2: Keras Tuner â€” Bayesian Optimization on Fashion-MNIST\n",
    "\n",
    "Explore systematic hyperparameter optimization using Keras Tuner. The principle behind it is quite\n",
    "similar to the implementation we did by hand, however it has support for other search algorithms\n",
    "than random search. Documentation can be found under: https://keras.io/keras_tuner/.\n",
    "\n",
    "\n",
    "\n",
    "Tasks:\n",
    "1. Choose and explain one optimization strategy:\n",
    "    - Either Hyperband Optimization\n",
    "    - Bayesian Optimization  \n",
    "\n",
    "    Briefly describe how the chosen method works (search for an appropriate reference paper or other academic resource! \n",
    "    Hint: look at what keras-tuner cites) and compare to random search.  \n",
    "\n",
    "2. Implement the search:\n",
    "Use Keras Tuner with your chosen strategy on the Fashion MNIST dataset\n",
    "Build a small comparison experiment with random search from exercise 1. (e.g. convergence\n",
    "speed.)\n",
    "Is such an approach inherently better than random search with additional manual tuning?\n",
    "Reason in one sentence.\n",
    "\n",
    "\n",
    "Deliverables: A notebook or script+markdown demonstrating your implementation with clear\n",
    "explanations and a summary of your findings. Note any use of an LLM in detail please.\n",
    "\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7140fb8",
   "metadata": {},
   "source": [
    "## How Bayesian Optimization Works\n",
    "\n",
    "Bayesian Optimization is an iterative method for hyperparameter tuning that treats the validation performance as a probabilistic surrogate model, commonly a Gaussian Process. An acquisition function is then used to propose new hyperparameter settings by trading off exploration of uncertain regions and exploitation of areas likely to yield high performance. In contrast to random search, which samples hyperparameters independently, Bayesian Optimization utilizes information from previous evaluations to inform future trials, often resulting in greater sample efficiency (Garnett, 2015).\n",
    "\n",
    "References:\n",
    "[1] Garnett, R. (2015). \"Bayesian Optimization.\" Lecture notes, CSE 515T: Bayesian Methods in Machine Learning, Washington University in St. Louis. Available at: https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfd1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: install and import dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e8df72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "270d5816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 28, 28, 1), (12000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use fashin dataset\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train_full = (X_train_full.astype(\"float32\") / 255.0)[..., None]\n",
    "X_test = (X_test.astype(\"float32\") / 255.0)[..., None]\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, stratify=y_train_full, random_state=SEED\n",
    ")\n",
    "\n",
    "X_tr.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc114d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(28, 28, 1)))\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Tune number of units\n",
    "    units = hp.Int(\"units\", min_value=64, max_value=512, step=64)\n",
    "    model.add(layers.Dense(units, activation=\"relu\"))\n",
    "\n",
    "    # Tune dropout\n",
    "    dropout = hp.Float(\"dropout\", min_value=0.0, max_value=0.5, step=0.1)\n",
    "    model.add(layers.Dropout(dropout))\n",
    "\n",
    "    model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    # Tune learning rate\n",
    "    lr = hp.Float(\"learning_rate\", 1e-4, 1e-2, sampling=\"log\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce62cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner\n",
    "tuner = keras_tuner.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,\n",
    "    directory=\"kt_dir\",\n",
    "    project_name=\"fashion_mnist_bo\",\n",
    "    seed=SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf499f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 12s]\n",
      "val_accuracy: 0.8895000219345093\n",
      "\n",
      "Best val_accuracy So Far: 0.8913333415985107\n",
      "Total elapsed time: 00h 03m 24s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(\n",
    "    X_tr, y_tr,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e99e69ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 00m 10s]\n",
      "val_accuracy: 0.8838333487510681\n",
      "\n",
      "Best val_accuracy So Far: 0.8927500247955322\n",
      "Total elapsed time: 00h 03m 14s\n"
     ]
    }
   ],
   "source": [
    "random_tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,\n",
    "    directory=\"kt_dir\",\n",
    "    project_name=\"random_search\",\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "random_tuner.search(\n",
    "    X_tr, y_tr,\n",
    "    epochs=10,\n",
    "    validation_data=(X_val, y_val),\n",
    "    batch_size=128,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b62e135",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loren\\anaconda3\\envs\\NDL3\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 10 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL of Bayesian Optimization best model:\n",
      "Test accuracy: 0.8795999884605408\n",
      "Test loss: 0.33820289373397827\n",
      "\n",
      "EVAL of Random Search best model:\n",
      "Test accuracy: 0.8787999749183655\n",
      "Test loss: 0.3380807340145111\n"
     ]
    }
   ],
   "source": [
    "best_model_bo = tuner.get_best_models()[0]\n",
    "\n",
    "print(\"EVAL of Bayesian Optimization best model:\")\n",
    "test_loss, test_acc = best_model_bo.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test loss:\", test_loss)\n",
    "\n",
    "print(\"\\nEVAL of Random Search best model:\")\n",
    "best_model_rs = random_tuner.get_best_models()[0]\n",
    "test_loss, test_acc = best_model_rs.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "print(\"Test loss:\", test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e309916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters from Random Search:\n",
      " {'units': 256, 'dropout': 0.2, 'learning_rate': 0.0006752863927347823}\n",
      "Best hyperparameters from Bayesian Optimization:\n",
      " {'units': 512, 'dropout': 0.2, 'learning_rate': 0.000686485764629306}\n"
     ]
    }
   ],
   "source": [
    "## Compare the hyperparameters:\n",
    "\n",
    "random_best = random_tuner.get_best_hyperparameters(1)[0]\n",
    "bayes_best = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"Best hyperparameters from Random Search:\\n\", random_best.values)\n",
    "print(\"Best hyperparameters from Bayesian Optimization:\\n\", bayes_best.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a39d034",
   "metadata": {},
   "source": [
    "**Is Bayesian Optimization inherently better than random search? (one sentence)**   \n",
    "Bayesian Optimization is not inherently better than random search with additional manual tuning, but it can reach competitive hyperparameter configurations more efficiently by exploiting information from previous trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fdd1e6",
   "metadata": {},
   "source": [
    "Short info: Both Bayesian Optimization and Random Search achieved very similar accuracy, with Random Search slightly higher, and Random Search was a bit faster, taking about 3:14 min versus 3:24 min for Bayesian Optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e6be5a",
   "metadata": {},
   "source": [
    "**LLM usage:**\n",
    "\n",
    "- Used the keras docu for the coding part and also copilot auto completion.\n",
    "- for the Evaluation prints I just took ChatGPT cause i was lazy\n",
    "- And also for the interpretations / reasonings i used chatGPT to rewrite it with better words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NDL3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
